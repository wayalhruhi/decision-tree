# -*- coding: utf-8 -*-
"""17EC10062_Assignment1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XI27mF3Xjyy6CkPPboAUlN9cLl8gJU2j
"""

# import pandas as pd
import numpy as np
eps = np.finfo(float).eps
from numpy import log2 as log

from numpy import genfromtxt
my_data = genfromtxt('../input/data1_19.csv', delimiter=',',dtype=str)#for kaggle
# my_data = genfromtxt('../content/data1_19.csv', delimiter=',',dtype=str)#for colab

print("data : \n", my_data)

df = my_data[1:]

print("shape : ", df[:, 1].shape)

def entropy(target_col):

    elements,counts = np.unique(target_col,return_counts = True)
    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])
    return entropy

data = df
data[data[:, 0]=='1st']

np.argmax(np.unique(data[:, 0],return_counts=True)[1])

print(df)

########iNFO GAIN
def InfoGain(data,split_attribute_name,target_name=3):
    
    """
    Calculate the information gain of a dataset. This function takes three parameters:
    1. data = The dataset for whose feature the IG should be calculated
    2. split_attribute_name = the name of the feature for which the information gain should be calculated
    3. target_name = the name of the target feature. The default for this example is "class"
    """
    
    #Calculate the entropy of the total dataset
    total_entropy = entropy(data[:, target_name])
    
    ##Calculate the entropy of the dataset
    
    #Calculate the values and the corresponding counts for the split attribute 
    vals,counts= np.unique(data[:, split_attribute_name],return_counts=True)
    
    #Calculate the weighted entropy
    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data[data[:, split_attribute_name]==vals[i]][:, target_name]) for i in range(len(vals))])
    
    #Calculate the information gain
    Information_Gain = total_entropy - Weighted_Entropy
    return Information_Gain

f=['pclass','age','gender']

###################
from pprint import pprint

###################
def BUILD_tree(data,originaldata,features,target_attribute_name=3,parent_node_class = None):
    if len(np.unique(data[:, target_attribute_name])) <= 1:
        return np.unique(data[:, target_attribute_name])[0]
    elif len(data)==0:
        return np.unique(originaldata[:, target_attribute_name])[np.argmax(np.unique(originaldata[:, target_attribute_name],return_counts=True)[1])]
    elif len(features) ==0:
        return parent_node_class
    else:
        parent_node_class = np.unique(data[:, target_attribute_name])[np.argmax(np.unique(data[:, target_attribute_name],return_counts=True)[1])]
        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset
        best_feature_index = np.argmax(item_values)
        best_feature = features[best_feature_index]
        tree = {f[best_feature]:{}}
        features = [i for i in features if i != best_feature]        
        for value in np.unique(data[:, best_feature]):
            value = value
            sub_data = data[data[:, best_feature] == value]            
            subtree = BUILD_tree(sub_data,dataset,features,target_attribute_name,parent_node_class)
            tree[f[best_feature]][ value] = subtree
        return(tree)

training_data = df
dataset = df

tree = BUILD_tree(training_data,training_data, [0, 1, 2])

print("tree")

pprint(tree)

print("values", tree.values())

